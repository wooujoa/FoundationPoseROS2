import sys
sys.path.append('./FoundationPose')
sys.path.append('./FoundationPose/nvdiffrast')

import rclpy
from rclpy.node import Node
from estimater import *
import cv2
import numpy as np
import trimesh
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import Pose, PoseStamped
from std_msgs.msg import Bool
from cv_bridge import CvBridge
import argparse
import os
from scipy.spatial.transform import Rotation as R
from ultralytics import SAM
from cam_2_base_transform import *
import os
import tkinter as tk
from tkinter import Listbox, END, Button
import glob
from vision_msgs.msg import CropPose

# Save the original `__init__` and `register` methods
original_init = FoundationPose.__init__
original_register = FoundationPose.register

# Modify `__init__` to add `is_register` attribute
def modified_init(self, model_pts, model_normals, symmetry_tfs=None, mesh=None, scorer=None, refiner=None, glctx=None, debug=0, debug_dir='./FoundationPose'):
    original_init(self, model_pts, model_normals, symmetry_tfs, mesh, scorer, refiner, glctx, debug, debug_dir)
    self.is_register = False  # Initialize as False

# Modify `register` to set `is_register` to True when a pose is registered
def modified_register(self, K, rgb, depth, ob_mask, iteration):
    pose = original_register(self, K, rgb, depth, ob_mask, iteration)
    self.is_register = True  # Set to True after registration
    return pose

# Apply the modifications
FoundationPose.__init__ = modified_init
FoundationPose.register = modified_register

# GUI ê¸°ë°˜ ë©”ì‹œ ìˆœì„œ ì„ íƒê¸° (ì´ˆê¸°í™” ì‹œ í˜¸ì¶œë¨)
class FileSelectorGUI:
    def __init__(self, master, file_paths):
        self.master = master
        self.master.title("Library: Sequence Selector")
        self.file_paths = file_paths
        self.reordered_paths = None  # Store the reordered paths here

        # ë©”ì‹œ íŒŒì¼ ì´ë¦„ë“¤ì„ ë³´ì—¬ì£¼ëŠ” ë¦¬ìŠ¤íŠ¸ ë°•ìŠ¤ ìƒì„±
        self.listbox = Listbox(master, selectmode="extended", width=50, height=10)
        self.listbox.pack()

        # ë¦¬ìŠ¤íŠ¸ë°•ìŠ¤ì— íŒŒì¼ ì´ë¦„ ì¶”ê°€
        for file_path in self.file_paths:
            file_name = os.path.splitext(os.path.basename(file_path))[0]
            self.listbox.insert(END, file_name)

        # ë²„íŠ¼ ê¸°ëŠ¥ë“¤
        self.up_button = Button(master, text="Move Up", command=self.move_up)
        self.up_button.pack(side="left", padx=5, pady=5)
        self.down_button = Button(master, text="Move Down", command=self.move_down)
        self.down_button.pack(side="left", padx=5, pady=5)
        self.done_button = Button(master, text="Done", command=self.done)
        self.done_button.pack(side="left", padx=5, pady=5)

    # ë²„íŠ¼ ê¸°ëŠ¥ í•¨ìˆ˜
    def move_up(self):
        """Move selected items up in the listbox."""
        selected_indices = list(self.listbox.curselection())
        for index in selected_indices:
            if index > 0:
                # Swap with the previous item
                file_name = self.listbox.get(index)
                self.listbox.delete(index)
                self.listbox.insert(index - 1, file_name)
                self.listbox.selection_set(index - 1)

    # ë²„íŠ¼ ê¸°ëŠ¥ í•¨ìˆ˜
    def move_down(self):
        """Move selected items down in the listbox."""
        selected_indices = list(self.listbox.curselection())
        for index in reversed(selected_indices):
            if index < self.listbox.size() - 1:
                # Swap with the next item
                file_name = self.listbox.get(index)
                self.listbox.delete(index)
                self.listbox.insert(index + 1, file_name)
                self.listbox.selection_set(index + 1)

    # ì„¤ì •í•œ ìˆœì„œëŒ€ë¡œ íŒŒì¼ ê²½ë¡œ ì¬êµ¬ì„± í›„ ì¢…ë£Œ
    def done(self):
        """Save the reordered paths and close the GUI."""
        reordered_file_names = self.listbox.get(0, END)

        # Recreate the full file paths based on the reordered file names (without extensions)
        file_name_to_full_path = {
            os.path.splitext(os.path.basename(file))[0]: file for file in self.file_paths
        }
        self.reordered_paths = [file_name_to_full_path[file_name] for file_name in reordered_file_names]

        # Close the GUI
        self.master.quit()

    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ìˆœì„œì˜ ì „ì²´ ê²½ë¡œ ë°˜í™˜
    def get_reordered_paths(self):
        """Return the reordered file paths after the GUI has closed."""
        return self.reordered_paths

# GUI ë„ìš°ê³  ìˆœì„œ ì„ íƒí•˜ê²Œ í•˜ëŠ” í•¨ìˆ˜ì„
def rearrange_files(file_paths):
    root = tk.Tk()
    app = FileSelectorGUI(root, file_paths)
    root.mainloop()  
    return app.get_reordered_paths()  # Return the reordered paths after GUI closes

# ëª…ë ¹ì¤„ ì¸ì íŒŒì„œ ì„¤ì •
parser = argparse.ArgumentParser()
code_dir = os.path.dirname(os.path.realpath(__file__))

# ê°ì²´ ë“±ë¡ ë° íŠ¸ë˜í‚¹ refinement ë°˜ë³µ íšŸìˆ˜ ì¸ì ë“±ë¡
parser.add_argument('--est_refine_iter', type=int, default=4)
parser.add_argument('--track_refine_iter', type=int, default=2)
args = parser.parse_args()

# ë©”ì¸ ë…¸ë“œ í´ë˜ìŠ¤, Pose + SAM2 + UIí†µí•œ ê°ì²´ ì„ íƒ + ROS PUB
class PoseEstimationNode(Node):
    def __init__(self, new_file_paths):
        super().__init__('pose_estimation_node')
        
        ## Foundation í™œì„±í™” í† í”½ ì¶”ê°€
        self.activate_topic = '/foundation_activate'
        
        # ROS2 í† í”½ êµ¬ë…ì/í¼ë¸”ë¦¬ì…” ì„¤ì •
        ## Foundation í™œì„±í™” ì‹ í˜¸ë¥¼ ë°›ëŠ” subscriber ì¶”ê°€
        self.activate_sub = self.create_subscription(Bool, self.activate_topic, self.activate_callback, 10)
        self.image_sub = self.create_subscription(Image, '/camera/camera/color/image_raw', self.image_callback, 10)
        self.depth_sub = self.create_subscription(Image, '/camera/camera/aligned_depth_to_color/image_raw', self.depth_callback, 10)
        self.info_sub = self.create_subscription(CameraInfo, '/camera/camera/color/camera_info', self.camera_info_callback, 10)
        
        # ë‚´ë¶€ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.bridge = CvBridge()
        self.depth_image = None
        self.color_image = None
        self.cam_K = None  # Initialize cam_K as None until we receive the camera info
        
        ## í™œì„±í™” ìƒíƒœ í”Œë˜ê·¸ ì¶”ê°€ - ì´ˆê¸°ê°’ì€ False (ë¹„í™œì„±í™”)
        self.activated = False
        ## ë“±ë¡ ì™„ë£Œ ì—¬ë¶€ í”Œë˜ê·¸ ì¶”ê°€ (í•œ ë²ˆë§Œ ë“±ë¡í•˜ê³  ì´í›„ íŠ¸ë˜í‚¹)
        self.registration_complete = False
        ## ìµœê·¼ í™œì„±í™” ì‹œê°„ ì¶”ì  (ìƒˆë¡œìš´ í™œì„±í™” ì‹ í˜¸ êµ¬ë¶„ìš©)
        self.last_activation_time = None
        
        # ğŸ”¹ ìƒˆë¡œ ì¶”ê°€: GUIë¥¼ í†µí•œ ìˆ˜ë™ ë°œí–‰ ì œì–´ í”Œë˜ê·¸
        self.manual_publish_mode = True  # ìˆ˜ë™ ë°œí–‰ ëª¨ë“œ í™œì„±í™”
        self.current_pose = None  # í˜„ì¬ ì¶”ì •ëœ í¬ì¦ˆ ì €ì¥
        
        # 3D ëª¨ë¸ ë¡œë”© ë° FoundationPose ì„¤ì •
        self.mesh_files = new_file_paths
        self.meshes = [trimesh.load(mesh) for mesh in self.mesh_files]
        self.bounds = [trimesh.bounds.oriented_bounds(mesh) for mesh in self.meshes]
        self.bboxes = [np.stack([-extents/2, extents/2], axis=0).reshape(2, 3) for _, extents in self.bounds]
        self.scorer = ScorePredictor()
        self.refiner = PoseRefinePredictor()
        self.glctx = dr.RasterizeCudaContext()

        # SAM2 ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”©
        self.seg_model = SAM("sam2.1_b.pt")

        # ì €ì¥ìš© ë³€ìˆ˜ ì„ ì–¸
        self.pose_estimations = {}  # Dictionary to track multiple pose estimations
        self.pose_publishers = {}  # Dictionary to store publishers for each object
        self.tracked_objects = []  # Initialize to store selected objects' masks
        self.i = 0

        self.frame_counter = 0
        
        ## ì‹œì‘ ë©”ì‹œì§€ ë³€ê²½ - í™œì„±í™” ëŒ€ê¸° ìƒíƒœì„ì„ ëª…ì‹œ
        self.get_logger().info("Foundation Pose Node Started - Waiting for activation signal...")

    ## ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜ - Foundation í™œì„±í™” ì‹ í˜¸ ë°›ëŠ” ì½œë°±
    def activate_callback(self, msg):
        """Foundation í™œì„±í™” ì‹ í˜¸ ë°›ëŠ” ì½œë°±"""
        current_time = self.get_clock().now()
        
        if msg.data and not self.activated:
            self.activated = True
            self.last_activation_time = current_time
            self.get_logger().info("Foundation ë…¸ë“œ í™œì„±í™”ë¨! 6D í¬ì¦ˆ ì¶”ì • ì‹œì‘...")
            
            # ğŸ”¹ ìƒˆë¡œìš´ í™œì„±í™” ì‹œ ë°œí–‰ ìƒíƒœ ì´ˆê¸°í™”
            self.current_pose = None
            
            # ìƒˆë¡œìš´ í™œì„±í™” ì‹œì—ë§Œ ë“±ë¡ ìƒíƒœ ì´ˆê¸°í™”
            if not self.registration_complete:
                self.reset_data()
                
        elif not msg.data and self.activated:
            self.activated = False
            self.get_logger().info("Foundation ë…¸ë“œ ë¹„í™œì„±í™”ë¨")
            # ë¹„í™œì„±í™” ì‹œì—ëŠ” ë“±ë¡ ìƒíƒœë¥¼ ìœ ì§€í•˜ì—¬ ë‹¤ìŒ í™œì„±í™” ì‹œ ì¦‰ì‹œ íŠ¸ë˜í‚¹ ê°€ëŠ¥

    ## ìƒˆë¡œ ì¶”ê°€ëœ í•¨ìˆ˜ - ë°ì´í„° ì´ˆê¸°í™” (ìƒˆë¡œìš´ ê³¼ì¼ ë“±ë¡ ì‹œì—ë§Œ)
    def reset_data(self):
        """ë°ì´í„° ì´ˆê¸°í™” (ìƒˆë¡œìš´ ê³¼ì¼ ë“±ë¡ ì‹œì—ë§Œ)"""
        self.depth_image = None
        self.color_image = None
        self.pose_estimations = {}
        self.tracked_objects = []
        self.i = 0
        self.registration_complete = False
        self.frame_counter = 0
        # ğŸ”¹ ë°œí–‰ ìƒíƒœë„ ì´ˆê¸°í™”
        self.current_pose = None
        self.get_logger().info("ìƒˆë¡œìš´ ê³¼ì¼ ë“±ë¡ì„ ìœ„í•œ ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")

    ## ì™„ì „ ì´ˆê¸°í™” í•¨ìˆ˜ (ëª¨ë“  ê³¼ì¼ ìˆ˜í™• ì™„ë£Œ ì‹œ)
    def full_reset(self):
        """ì™„ì „ ì´ˆê¸°í™” (ëª¨ë“  ê³¼ì¼ ìˆ˜í™• ì™„ë£Œ ì‹œ)"""
        self.reset_data()
        # ë©”ì‹œì™€ ë°”ìš´ë“œë„ ì´ˆê¸° ìƒíƒœë¡œ ë³µì›
        self.meshes = [trimesh.load(mesh) for mesh in self.mesh_files]
        self.bounds = [trimesh.bounds.oriented_bounds(mesh) for mesh in self.meshes]
        self.get_logger().info("ëª¨ë“  ë°ì´í„° ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")

    def project_point(self, pt3d, K):
        x, y, z = pt3d
        if z <= 0:
            return None
        fx, fy = K[0, 0], K[1, 1]
        cx, cy = K[0, 2], K[1, 2]
        u = fx * x / z + cx
        v = fy * y / z + cy
        return (u, v)
    
    # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ìˆ˜ì‹  ì½œë°±
    def camera_info_callback(self, msg):
        if self.cam_K is None:  # Update cam_K only once to avoid redundant updates
            self.cam_K = np.array(msg.k).reshape((3, 3))
            self.get_logger().info(f"Camera intrinsic matrix initialized: {self.cam_K}")

    # ì»¬ëŸ¬ ì´ë¯¸ì§€ ìˆ˜ì‹  ì½œë°±
    def image_callback(self, msg):
        ## í™œì„±í™” ìƒíƒœ ì²´í¬ ì¶”ê°€ - ë¹„í™œì„±í™” ìƒíƒœë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if not self.activated:
            return
        
        self.color_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        self.color_image = cv2.cvtColor(self.color_image, cv2.COLOR_BGR2RGB)

    # ëìŠ¤ ì´ë¯¸ì§€ ìˆ˜ì‹  ì½œë°±
    def depth_callback(self, msg):
        ## í™œì„±í™” ìƒíƒœ ì²´í¬ ì¶”ê°€ - ë¹„í™œì„±í™” ìƒíƒœë©´ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
        if not self.activated:
            return
        
        self.depth_image = self.bridge.imgmsg_to_cv2(msg, "32FC1") / 1e3
        self.process_images()

    # ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜, ê°ì²´ ë“±ë¡ ë° íŠ¸ë˜í‚¹ ì‹¤í–‰
    def process_images(self):
        ## í™œì„±í™” ìƒíƒœ ì¬í™•ì¸ - ì²˜ë¦¬ ì¤‘ì— ë¹„í™œì„±í™”ë˜ë©´ ì¤‘ë‹¨
        if not self.activated:
            return

        # ë“±ë¡ì´ ì™„ë£Œëœ ê²½ìš° ë°”ë¡œ íŠ¸ë˜í‚¹ ëª¨ë“œë¡œ (í•˜ì§€ë§Œ ë°œí–‰ì€ ì œì–´)
        if self.registration_complete and self.pose_estimations:
            self.perform_tracking()
            return

        # ìµœì†Œ 5í”„ë ˆì„ ëˆ„ì ë˜ê¸° ì „ì—ëŠ” ë“±ë¡ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
        self.frame_counter += 1
        if self.frame_counter < 5:
            self.get_logger().info(f"Waiting for registration... Frame count: {self.frame_counter}")
            return
        
        # ë“±ë¡ ê³¼ì • ìˆ˜í–‰
        self.perform_registration()

    def perform_registration(self):
        """ê°ì²´ ë“±ë¡ ê³¼ì •"""
        cv2.imwrite("debug_color.png", cv2.cvtColor(self.color_image, cv2.COLOR_RGB2BGR))  # ì‹œê° í™•ì¸ìš©

        if self.color_image is None or self.depth_image is None or self.cam_K is None:
            return
        
        H, W = self.color_image.shape[:2]
        color = cv2.resize(self.color_image, (W, H), interpolation=cv2.INTER_NEAREST)
        depth = cv2.resize(self.depth_image, (W, H), interpolation=cv2.INTER_NEAREST)
        depth[(depth < 0.1) | (depth >= np.inf)] = 0

        if self.i == 0:
            masks_accepted = False

            while not masks_accepted and self.activated:  # í™œì„±í™” ìƒíƒœ ê³„ì† í™•ì¸
                # SAM ì„¸ê·¸ë©˜í…Œì´ì…˜
                sam_input = cv2.cvtColor(self.color_image, cv2.COLOR_RGB2BGR)
                res = self.seg_model.predict(sam_input)[0]
                res.save("masks.png")

                if not res:
                    self.get_logger().warn("No masks detected by SAM2.")
                    return

                objects_to_track = []

                # Iterate over the segmentation results to extract the masks and bounding boxes
                for r in res:
                    img = np.copy(r.orig_img)
                    for ci, c in enumerate(r):
                        mask = np.zeros((H, W), np.uint8)
                        contour = c.masks.xy.pop().astype(np.int32).reshape(-1, 1, 2)
                        _ = cv2.drawContours(mask, [contour], -1, (255, 255, 255), cv2.FILLED)

                        # Store mask and bounding box
                        objects_to_track.append({
                            'mask': mask,
                            'box': c.boxes.xyxy.tolist().pop(),
                            'contour': contour,
                            'orig_img': img
                        })

                if not objects_to_track:
                    self.get_logger().warn("No objects found in the image.")
                    return

                self.tracked_objects = []  # Reset tracked objects for redo
                temporary_pose_estimations = {}
                skipped_indices = []  # Track skipped objects' indices

                def click_event(event, x, y, flags, params):
                    if event == cv2.EVENT_LBUTTONDOWN:
                        closest_dist = float('inf')
                        selected_obj = None

                        for obj in objects_to_track:
                            if obj['mask'][y, x] == 255:  # Check if click is inside the mask
                                dist = cv2.pointPolygonTest(obj['contour'], (x, y), True)

                                if dist < closest_dist:
                                    closest_dist = dist
                                    selected_obj = obj

                        if selected_obj is not None:
                            sequential_id = len(self.tracked_objects) + len(skipped_indices)
                            self.get_logger().info(f"Object {sequential_id} selected.")
                            self.tracked_objects.append(selected_obj['mask'])

                            # Temporarily store the mesh and bounds to avoid permanent removal
                            temp_mesh = self.meshes.pop(0)  # Remove the first mesh in line
                            temp_to_origin, _ = self.bounds.pop(0)  # Remove the first bound in line

                            # Initialize FoundationPose for each detected object with corresponding mesh
                            pose_est = FoundationPose(
                                model_pts=temp_mesh.vertices,
                                model_normals=temp_mesh.vertex_normals,
                                mesh=temp_mesh,
                                scorer=self.scorer,
                                refiner=self.refiner,
                                glctx=self.glctx
                            )

                            temporary_pose_estimations[sequential_id] = {
                                'pose_est': pose_est,
                                'mask': selected_obj['mask'],
                                'to_origin': temp_to_origin
                            }

                            # Refresh the dialog box with the updated object name
                            refresh_dialog_box()

                def refresh_dialog_box():
                    # Display contours for all detected objects
                    combined_mask_image = cv2.cvtColor(np.copy(objects_to_track[0]['orig_img']), cv2.COLOR_BGR2RGB)
                    for idx, obj in enumerate(objects_to_track):
                        cv2.drawContours(combined_mask_image, [obj['contour']], -1, (0, 255, 0), 2)  # Green contours

                    # Get the next mesh name for user guidance, accounting for skips
                    next_mesh_idx = len(self.tracked_objects) + len(skipped_indices)
                    if next_mesh_idx < len(self.mesh_files):
                        next_mesh_name = os.path.basename(self.mesh_files[next_mesh_idx].split("/")[-1].split(".")[0])
                    else:
                        next_mesh_name = "None"

                    # Create the dialog box overlay
                    overlay = combined_mask_image.copy()
                    dialog_text = (
                        f"Next object to select: {next_mesh_name}\n"
                        "Instructions:\n"
                        "- Click on the object to select.\n"
                        "- Press 's' to skip the current object.\n"
                        "- Press 'c', 'Enter', or 'Space' to confirm selection.\n"
                        "- Press 'r' to redo mask selection.\n"
                        "- Press 'q' to quit.\n"
                    )
                    y0, dy = 30, 20
                    for i, line in enumerate(dialog_text.split('\n')):
                        y = y0 + i * dy
                        cv2.putText(overlay, line, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)

                    cv2.imshow('Click on objects to track', cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
                    cv2.setMouseCallback('Click on objects to track', click_event)

                refresh_dialog_box()

                while True and self.activated:  # í™œì„±í™” ìƒíƒœ ê³„ì† í™•ì¸
                    key = cv2.waitKey(0)  # Wait for a key event
                    if key == ord('r'):
                        self.get_logger().info("Redoing mask selection.")
                        break  # Break the inner loop to redo mask selection
                    elif key == ord('s'):
                        self.get_logger().info("Skipping current object.")
                        skipped_indices.append(len(self.tracked_objects) + len(skipped_indices))  # Track skipped mesh index

                        # Remove the first mesh and bounds in line
                        self.meshes.pop(0)
                        self.bounds.pop(0)

                        refresh_dialog_box()
                    elif key in [ord('q'), 27]:  # 'q' or Esc to quit
                        self.get_logger().info("Quitting mask selection.")
                        return
                    elif key in [ord('c'), 13, 32]:  # 'c', Enter, or Space to confirm
                        if self.tracked_objects:
                            # Confirm the selection and update the actual pose_estimations
                            self.pose_estimations = temporary_pose_estimations
                            masks_accepted = True  # Exit the outer loop if masks are accepted
                            break
                        else:
                            self.get_logger().warn("No objects selected. Redo mask selection.")

        # ë“±ë¡ ìˆ˜í–‰
        if self.pose_estimations:
            for idx, data in self.pose_estimations.items():
                pose_est = data['pose_est']
                obj_mask = data['mask']
                
                if not pose_est.is_register:
                    pose = pose_est.register(K=self.cam_K, rgb=color, depth=depth, ob_mask=obj_mask, iteration=args.est_refine_iter)
                    rotation_matrix = pose[:3, :3]
                    z_axis_vector = rotation_matrix[:, 2]
                    self.get_logger().info(f"[Object {idx}] Registered Z-axis direction: {z_axis_vector}")
                    
            self.registration_complete = True
            self.get_logger().info("ê°ì²´ ë“±ë¡ ì™„ë£Œ! íŠ¸ë˜í‚¹ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.i += 1

    def perform_tracking(self):
        """íŠ¸ë˜í‚¹ ëª¨ë“œ ì‹¤í–‰ - ë§¤ë²ˆ 'p' í‚¤ë¥¼ ëˆ„ë¥¼ ë•Œë§ˆë‹¤ ë°œí–‰ ê°€ëŠ¥"""
        if self.color_image is None or self.depth_image is None or self.cam_K is None:
            return
            
        H, W = self.color_image.shape[:2]
        color = cv2.resize(self.color_image, (W, H), interpolation=cv2.INTER_NEAREST)
        depth = cv2.resize(self.depth_image, (W, H), interpolation=cv2.INTER_NEAREST)
        depth[(depth < 0.1) | (depth >= np.inf)] = 0

        visualization_image = np.copy(color)

        # íŠ¸ë˜í‚¹ ìˆ˜í–‰ (ì‹œê°í™”ìš©)
        for idx, data in self.pose_estimations.items():
            pose_est = data['pose_est']
            to_origin = data['to_origin']
            
            if pose_est.is_register:
                pose = pose_est.track_one(rgb=color, depth=depth, K=self.cam_K, iteration=args.track_refine_iter)
                center_pose = pose @ np.linalg.inv(to_origin)
                self.current_pose = center_pose  # í˜„ì¬ í¬ì¦ˆ ì—…ë°ì´íŠ¸

                # ì‹œê°í™” ìˆ˜í–‰
                visualization_image = self.visualize_pose(visualization_image, center_pose, idx)

        # ğŸ”¹ GUI ì»¨íŠ¸ë¡¤ ì œê±° - ê¹”ë”í•œ ì‹œê°í™”ë¥¼ ìœ„í•´ ìƒë‹¨ ì •ë³´ íŒ¨ë„ ì œê±°

        cv2.imshow('Pose Estimation & Tracking', visualization_image[..., ::-1])
        
        # ğŸ”¹ í‚¤ ì…ë ¥ ì²˜ë¦¬ - 'p' í‚¤ë¥¼ ëˆ„ë¥¼ ë•Œë§ˆë‹¤ ë°œí–‰
        key = cv2.waitKey(1) & 0xFF
        if key == ord('p') and self.current_pose is not None:
            self.publish_crop_pose(self.current_pose, topic_name="/CropPose/obj")
            self.get_logger().info("=== 'P' í‚¤ ì…ë ¥ìœ¼ë¡œ CropPose ë°œí–‰! ===")
        elif key == ord('q'):
            self.get_logger().info("=== 'Q' í‚¤ë¡œ ì¢…ë£Œ ìš”ì²­ ===")
            # í•„ìš”ì‹œ ì¢…ë£Œ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€

    def add_publish_control_gui(self, image, center_pose):
        """GUI ì»¨íŠ¸ë¡¤ ì œê±°ë¨ - ê¹”ë”í•œ ì‹œê°í™”ë¥¼ ìœ„í•´"""
        return image  # ì•„ë¬´ ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜

    # ğŸ”¹ ìˆ˜ì •ëœ ì‹œê°í™” í•¨ìˆ˜ - Zì¶•ë§Œ í‘œì‹œí•˜ê³  cutting pointë¥¼ ëª…í™•í•˜ê²Œ í‘œì‹œ
    def visualize_pose(self, image, center_pose, idx):
        vis = image.copy()
        
        # 1. Zì¶•ë§Œ í‘œì‹œ (íŒŒë€ìƒ‰)
        origin = np.array([0, 0, 0, 1])
        z_axis_end = np.array([0, 0, 0.05, 1])  # Zì¶• ìœ„ë¡œ
        
        origin_cam = center_pose @ origin
        z_end_cam = center_pose @ z_axis_end
        
        # ì¢Œí‘œ íˆ¬ì˜
        origin_2d = self.project_point(origin_cam[:3], self.cam_K)
        z_end_2d = self.project_point(z_end_cam[:3], self.cam_K)
        
        if origin_2d is not None and z_end_2d is not None:
            origin_2d = tuple(map(int, origin_2d))
            z_end_2d = tuple(map(int, z_end_2d))
            
            # Zì¶• í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰, ë‘êº¼ìš´ ì„ )
            cv2.arrowedLine(vis, origin_2d, z_end_2d, (255, 0, 0), 4, tipLength=0.3)
            
            # Zì¶• ë¼ë²¨
            cv2.putText(vis, "Z", (z_end_2d[0] + 10, z_end_2d[1] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2, cv2.LINE_AA)

        # 2. Cutting Point ëª…í™•í•˜ê²Œ í‘œì‹œ
        offset = np.array([0.00, 0.00, 0.05, 1])  # ì ˆë‹¨ì  ì˜¤í”„ì…‹
        cutting_point_cam = center_pose @ offset
        cutting_point_2d = self.project_point(cutting_point_cam[:3], self.cam_K)
        
        if cutting_point_2d is not None:
            cutting_point_2d = tuple(map(int, cutting_point_2d))
            
            # ì ˆë‹¨ì ì„ í° ì›ìœ¼ë¡œ í‘œì‹œ (ë¹¨ê°„ìƒ‰)
            cv2.circle(vis, cutting_point_2d, 8, (0, 0, 255), -1)  # ì±„ì›Œì§„ ì›
            cv2.circle(vis, cutting_point_2d, 12, (255, 255, 255), 2)  # í°ìƒ‰ í…Œë‘ë¦¬
            
            # ì ˆë‹¨ì  ë¼ë²¨
            cv2.putText(vis, "CUT", (cutting_point_2d[0] + 15, cutting_point_2d[1] - 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)
            
            # ì ˆë‹¨ì ê¹Œì§€ì˜ ê±°ë¦¬ í‘œì‹œ
            cutting_distance = np.linalg.norm(cutting_point_cam[:3])
            cv2.putText(vis, f"{cutting_distance:.3f}m", 
                       (cutting_point_2d[0] + 15, cutting_point_2d[1] + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)

        # 3. ê°ì²´ ì¤‘ì‹¬ì  í‘œì‹œ (ì´ˆë¡ìƒ‰)
        if origin_2d is not None:
            cv2.circle(vis, origin_2d, 6, (0, 255, 0), -1)  # ì±„ì›Œì§„ ì›
            cv2.circle(vis, origin_2d, 10, (255, 255, 255), 2)  # í°ìƒ‰ í…Œë‘ë¦¬
            
            # ê°ì²´ ì¤‘ì‹¬ ë¼ë²¨
            cv2.putText(vis, "CENTER", (origin_2d[0] + 15, origin_2d[1] - 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)

        # 4. ì‹œê°í™” ì™„ë£Œ - ê±°ë¦¬ ì •ë³´ íŒ¨ë„ ì œê±°
        # vis = self.draw_distance_info(vis, center_pose, origin_2d)  # ì œê±°ë¨

        return vis

    # ğŸ”¹ ìˆ˜ì •ëœ í•¨ìˆ˜: QoS í˜¸í™˜ì„±ì„ ìœ„í•´ transient_local ì‚¬ìš©
    def publish_crop_pose(self, center_pose, topic_name="/CropPose/obj"):
        if topic_name not in self.pose_publishers:
            # ğŸ”¹ QoS ì„¤ì • ì¶”ê°€ - Master ë…¸ë“œì™€ í˜¸í™˜ë˜ë„ë¡ transient_local ì‚¬ìš©
            from rclpy.qos import QoSProfile, QoSDurabilityPolicy, QoSReliabilityPolicy
            
            qos_profile = QoSProfile(
                depth=10,
                durability=QoSDurabilityPolicy.TRANSIENT_LOCAL,  # Master ë…¸ë“œì™€ í˜¸í™˜
                reliability=QoSReliabilityPolicy.RELIABLE
            )
            
            self.pose_publishers[topic_name] = self.create_publisher(
                CropPose, 
                topic_name, 
                qos_profile
            )
            self.get_logger().info(f"Publisher ìƒì„±ë¨: {topic_name} (QoS: transient_local, reliable)")

        # ê°ì²´ ì¤‘ì‹¬ì ì˜ ì¹´ë©”ë¼ ì¢Œí‘œê³„ ìœ„ì¹˜
        object_center = center_pose @ np.array([0, 0, 0, 1])
        
        # Zì¶• ë°©í–¥ìœ¼ë¡œ ì˜¤í”„ì…‹ (ì¤„ê¸° ë°©í–¥)
        offset = np.array([0.00, 0.00, 0.02, 1])  # Zì¶• ìœ„ë¡œ
        cutting_point = center_pose @ offset

        msg = CropPose()
        # ë¯¸í„° ë‹¨ìœ„ë¡œ ì§ì ‘ ë°œí–‰ (ë³€í™˜ ì—†ìŒ)
        msg.x = cutting_point[0]  # ë¯¸í„° ë‹¨ìœ„
        msg.y = cutting_point[1]  # ë¯¸í„° ë‹¨ìœ„
        msg.z = cutting_point[2]  # ë¯¸í„° ë‹¨ìœ„ (ì¹´ë©”ë¼ ì¢Œí‘œê³„ Zì¶• = ê¹Šì´)

        # í˜„ì¬ ì‹œê°„ ì¶”ê°€
        current_time = self.get_clock().now()
        
        # ë””ë²„ê¹…ìš© ì •ë³´ ì¶œë ¥ (ë¯¸í„° ë‹¨ìœ„)
        object_distance = np.linalg.norm(object_center[:3])  # ì¹´ë©”ë¼-ê°ì²´ ì§ì„ ê±°ë¦¬
        cutting_distance = np.linalg.norm(cutting_point[:3])  # ì¹´ë©”ë¼-ì ˆë‹¨ì  ì§ì„ ê±°ë¦¬
        
        self.get_logger().info(f"=== CropPose ë°œí–‰ë¨! [{current_time.nanoseconds // 1000000}ms] ===")
        self.get_logger().info(f"ê°ì²´ ì¤‘ì‹¬: X={object_center[0]:.3f}m, Y={object_center[1]:.3f}m, Z={object_center[2]:.3f}m")
        self.get_logger().info(f"ì ˆë‹¨ì : X={msg.x:.3f}m, Y={msg.y:.3f}m, Z={msg.z:.3f}m")
        self.get_logger().info(f"ê°ì²´ê¹Œì§€ ì§ì„ ê±°ë¦¬: {object_distance:.3f}m")
        self.get_logger().info(f"ì ˆë‹¨ì ê¹Œì§€ ì§ì„ ê±°ë¦¬: {cutting_distance:.3f}m")
        self.get_logger().info(f"Zì¶• ê¹Šì´ (ì¹´ë©”ë¼ ì¢Œí‘œ): {cutting_point[2]:.3f}m")

        # ğŸ”¹ ì‹¤ì œ ë©”ì‹œì§€ ë°œí–‰
        self.pose_publishers[topic_name].publish(msg)
        self.get_logger().info(f"=== CropPose ë©”ì‹œì§€ê°€ {topic_name} í† í”½ìœ¼ë¡œ ë°œí–‰ë˜ì—ˆìŠµë‹ˆë‹¤! (QoS: transient_local) ===")
        self.get_logger().info(f"=== ë‹¤ì‹œ 'P' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¬ë°œí–‰ë©ë‹ˆë‹¤ ===")

    def draw_camera_coordinate_system(self, image, center_pose):
        """ì¹´ë©”ë¼ ì›ì ê³¼ ì¢Œí‘œê³„ë¥¼ ì´ë¯¸ì§€ì— í‘œì‹œ"""
        vis = image.copy()
        
        # ì¹´ë©”ë¼ ì›ì  (0, 0, 0)ì€ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ
        # ì´ë¯¸ì§€ ì¤‘ì•™ì— ì¹´ë©”ë¼ ì¢Œí‘œê³„ ì •ë³´ë¥¼ í‘œì‹œ
        h, w = vis.shape[:2]
        
        # 1. ì´ë¯¸ì§€ ì¤‘ì‹¬ì— ì¹´ë©”ë¼ ì›ì  í‘œì‹œ
        camera_center = (w // 2, h // 2)
        
        # ì¹´ë©”ë¼ ì›ì  ë§ˆì»¤ (í° ì‹­ìê°€)
        cv2.drawMarker(vis, camera_center, (0, 255, 255), markerType=cv2.MARKER_CROSS, 
                      markerSize=30, thickness=3)
        
        # ì¹´ë©”ë¼ ì›ì  í…ìŠ¤íŠ¸
        cv2.putText(vis, "Camera Origin (0,0,0)", 
                   (camera_center[0] - 80, camera_center[1] - 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3, cv2.LINE_AA)
        cv2.putText(vis, "Camera Origin (0,0,0)", 
                   (camera_center[0] - 80, camera_center[1] - 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)
        
        # 2. ì¹´ë©”ë¼ ì¢Œí‘œì¶• ë°©í–¥ í‘œì‹œ (ì´ë¯¸ì§€ ëª¨ì„œë¦¬ì—)
        axis_length = 50
        
        # Xì¶• (ì˜¤ë¥¸ìª½, ë¹¨ê°„ìƒ‰)
        cv2.arrowedLine(vis, (50, h - 100), (50 + axis_length, h - 100), (0, 0, 255), 3)
        cv2.putText(vis, "X+", (50 + axis_length + 5, h - 95), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # Yì¶• (ì•„ë˜, ì´ˆë¡ìƒ‰)
        cv2.arrowedLine(vis, (50, h - 100), (50, h - 100 + axis_length), (0, 255, 0), 3)
        cv2.putText(vis, "Y+", (55, h - 100 + axis_length + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # Zì¶• ì„¤ëª… (í™”ë©´ ë°–ìœ¼ë¡œ, íŒŒë€ìƒ‰)
        cv2.putText(vis, "Z+ (into screen)", (50, h - 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        # 3. ê´‘ì¶• í‘œì‹œ (ì´ë¯¸ì§€ ì¤‘ì‹¬ì—ì„œ ê°ì²´ë¡œì˜ ì—°ê²°ì„ )
        object_center = center_pose @ np.array([0, 0, 0, 1])
        object_2d = self.project_point(object_center[:3], self.cam_K)
        
        if object_2d is not None:
            object_2d = tuple(map(int, object_2d))
            # ì¹´ë©”ë¼ ì¤‘ì‹¬ì—ì„œ ê°ì²´ê¹Œì§€ì˜ ì—°ê²°ì„  (ì ì„ )
            self.draw_dashed_line(vis, camera_center, object_2d, (128, 128, 128), 2)
            
            # ê´‘ì¶• í…ìŠ¤íŠ¸
            mid_x = (camera_center[0] + object_2d[0]) // 2
            mid_y = (camera_center[1] + object_2d[1]) // 2
            cv2.putText(vis, "Optical Ray", (mid_x - 40, mid_y - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 2)
        
        return vis
    
    def draw_dashed_line(self, image, pt1, pt2, color, thickness):
        """ì ì„  ê·¸ë¦¬ê¸°"""
        dist = np.sqrt((pt2[0] - pt1[0])**2 + (pt2[1] - pt1[1])**2)
        pts = []
        for i in np.arange(0, dist, 10):  # 10í”½ì…€ ê°„ê²©
            r = i / dist
            x = int((1 - r) * pt1[0] + r * pt2[0])
            y = int((1 - r) * pt1[1] + r * pt2[1])
            pts.append((x, y))
        
        for i in range(0, len(pts) - 1, 2):  # 2ê°œì”© ê±´ë„ˆë›°ë©° ì ì„  íš¨ê³¼
            if i + 1 < len(pts):
                cv2.line(image, pts[i], pts[i + 1], color, thickness)
    
    def draw_distance_info(self, image, center_pose, object_2d_pos):
        """ê±°ë¦¬ ì •ë³´ íŒ¨ë„ ì œê±°ë¨ - ê¹”ë”í•œ ì‹œê°í™”ë¥¼ ìœ„í•´"""
        return image  # ì•„ë¬´ ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜
        
    # ì¶”ê°€: ì¹´ë©”ë¼ ì¢Œí‘œê³„ í™•ì¸ í•¨ìˆ˜
    def verify_camera_coordinate_system(self):
        """ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë°©í–¥ í™•ì¸"""
        if self.cam_K is not None:
            fx, fy = self.cam_K[0, 0], self.cam_K[1, 1]
            cx, cy = self.cam_K[0, 2], self.cam_K[1, 2]
            
            self.get_logger().info(f"=== ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ===")
            self.get_logger().info(f"ì´ˆì ê±°ë¦¬: fx={fx:.1f}, fy={fy:.1f}")
            self.get_logger().info(f"ì£¼ì : cx={cx:.1f}, cy={cy:.1f}")
            self.get_logger().info(f"ì¹´ë©”ë¼ ì¢Œí‘œê³„: X=ì˜¤ë¥¸ìª½, Y=ì•„ë˜, Z=ì¹´ë©”ë¼ì—ì„œ ë©€ì–´ì§€ëŠ” ë°©í–¥(ê¹Šì´)")

    # ì¢Œí‘œ ë³€í™˜ ê²€ì¦ í•¨ìˆ˜ ì¶”ê°€
    def validate_depth_calculation(self, center_pose):
        """ê¹Šì´ ê³„ì‚° ê²€ì¦"""
        
        # 1. ê°ì²´ ì¤‘ì‹¬ì˜ ì¹´ë©”ë¼ ì¢Œí‘œ
        object_center = center_pose @ np.array([0, 0, 0, 1])
        
        # 2. ì¹´ë©”ë¼ ì›ì ì—ì„œ ê°ì²´ê¹Œì§€ì˜ ë²¡í„°
        distance_vector = object_center[:3]
        
        # 3. ì§ì„ ê±°ë¦¬ vs Zì¶• ê¹Šì´ ë¹„êµ
        euclidean_distance = np.linalg.norm(distance_vector)  # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        z_depth = object_center[2]  # Zì¶• ê¹Šì´
        
        # 4. ì°¸ì™¸ê°€ ì¹´ë©”ë¼ ì •ë©´ì— ìˆë‹¤ë©´ euclidean_distance â‰ˆ z_depth ì´ì–´ì•¼ í•¨
        depth_difference = abs(euclidean_distance - z_depth)
        
        self.get_logger().info(f"=== ê¹Šì´ ê³„ì‚° ê²€ì¦ (ë¯¸í„° ë‹¨ìœ„) ===")
        self.get_logger().info(f"ìœ í´ë¦¬ë“œ ê±°ë¦¬: {euclidean_distance:.3f}m")
        self.get_logger().info(f"Zì¶• ê¹Šì´: {z_depth:.3f}m")
        self.get_logger().info(f"ì°¨ì´: {depth_difference:.3f}m")
        
        if depth_difference > 0.05:  # 5cm ì´ìƒ ì°¨ì´
            self.get_logger().warn(f"ê¹Šì´ ê³„ì‚° ë¶ˆì¼ì¹˜! ê°ì²´ê°€ ì¹´ë©”ë¼ ì •ë©´ì— ìˆì§€ ì•Šì„ ê°€ëŠ¥ì„±")
            self.get_logger().info(f"ê°ì²´ ìœ„ì¹˜ ë²¡í„°: [{distance_vector[0]:.3f}, {distance_vector[1]:.3f}, {distance_vector[2]:.3f}]m")
        
        return euclidean_distance, z_depth


def main(args=None):
    source_directory = "demo_data"
    file_paths = glob.glob(os.path.join(source_directory, '**', '*.obj'), recursive=True) + \
                 glob.glob(os.path.join(source_directory, '**', '*.stl'), recursive=True) + \
                 glob.glob(os.path.join(source_directory, '**', '*.STL'), recursive=True)

    # Call the function to rearrange files through the GUI
    new_file_paths = rearrange_files(file_paths)

    rclpy.init(args=args)
    node = PoseEstimationNode(new_file_paths)
    ## ë…¸ë“œê°€ ê³„ì† ì‹¤í–‰ë˜ì–´ ì—¬ëŸ¬ ë²ˆ í™œì„±í™” ì‹ í˜¸ë¥¼ ë°›ì„ ìˆ˜ ìˆë„ë¡ ìœ ì§€
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()


if __name__ == '__main__':
    main()